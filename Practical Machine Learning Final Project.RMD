---
title: "Machine Learning Final Project"
author: "P Kundamal"
date: "July 30, 2017"
output:
  html_document: default
---


## Processing Raw Data and Load Relevant Packages. 

First, we download the training and test datasets and load them using read.csv function. 

```{r cache = TRUE, message=FALSE}
library(caret)
train <- read.csv("pml-training.csv", header = TRUE, na.strings=c("","NA", "#DIV/0!"))
test <- read.csv("pml-testing.csv", header = TRUE, na.strings=c("","NA", "#DIV/0!"))
```

In order to run the machine learning algorithms, the features used cannot contain any `NA` values. To see which variables/features should be used, I calculated the percentage of NA's for each column. 

```{r cache = TRUE}
# see error percentage 
NAPercent <- round(colMeans(is.na(train)), 2)
table(NAPercent)
```
From above, we can see that only 60 variables have complete data so those are the variables we will use to build the prediction algorithm. I removed the first variable here because it is the row index from the csv file and not a true variable.

```{r cache = TRUE}
# find index of the complete columns minus the first 
index <- which(NAPercent==0)[-1]
# subset the data
train <- train[, index]
test <- test[, index]
# looking at the structure of the data for the first 10 columns
str(train[, 1:10])
```
From the structure of the data, we can see that the first 6 variables are likely not useful for prediction modeling. We are going to leave those 6 columns out before we build the algorithm. In addition, to make the columns easier to deal with, we will go ahead and convert all features to `numeric` class.

```{r cache = TRUE}
# subset the data
train <- train[, -(1:6)]
test <- test[, -(1:6)]
# convert all numerical data to numeric class
for(i in 1:(length(train)-1)){
    train[,i] <- as.numeric(train[,i])
    test[,i] <- as.numeric(test[,i])
}
```

## Cross Validation

For this project, we will focus on using the two most widely-used, most accurate prediction algorithms, 

We set `test` set aside and split the `train` data into two sections for cross validation. We will allocate 80% of the data to train the model and 20% to validate it.

We expect that the error rates returned by the models should be good estimate for  the out of sample error rate. We will get actual estimates of error rates from the **accuracies** achieved by the models.

```{r cache = TRUE}
# split train data set
inTrain <- createDataPartition(y=train$classe,p=0.8, list=FALSE)
trainData <- train[inTrain,]
validation <- train[-inTrain,]
# print out the dimentions of the 3 data sets
rbind(trainData = dim(trainData), validation = dim(validation), test = dim(test))
```

## Comparing Model and Results 

We will use **random forest** to build the first model. 

```{r cache = TRUE, message=FALSE}
# load randomForest package
library(randomForest)
# run the random forest algorithm on the training data set
rfFit <- randomForest(classe~., data = trainData, method ="rf", prox = TRUE)
rfFit
# use model to predict on validation data set
rfPred <- predict(rfFit, validation)

```


From the above, we can see that randomForest is the better performing algorithm with **0.43%** out-of-bag (OOB) error rate. When applied to the validation set for cross validation, the model achieved an accuracy of **99.7%**, which indicates the actual error rate is **0.3%**


## Result

We can apply the randomForest model to the 20 given test set for the predictions. 

```{r cache = TRUE}
# apply random forest model to test set
predict(rfFit, test)
```



